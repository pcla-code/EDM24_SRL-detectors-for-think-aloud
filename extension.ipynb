{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9e6caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import openai\n",
    "\n",
    "from sklearn.metrics import classification_report, roc_curve, roc_auc_score, confusion_matrix, accuracy_score, f1_score, cohen_kappa_score\n",
    "from sklearn.model_selection import GroupKFold, train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn import tree, metrics\n",
    "from sklearn.utils import resample\n",
    "import xgboost\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, precision_score, recall_score\n",
    "from sklearn.utils import resample\n",
    "\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "from tensorflow.keras.optimizers import Adam, SGD, RMSprop, Adadelta, Adagrad, Adamax, Nadam, Ftrl\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "random.seed(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e92332-28a0-4263-9540-69904331e516",
   "metadata": {},
   "source": [
    "## Label Frequency\n",
    "\n",
    "The code below reads in the relevant ground-truth SRL label files including associated student utterances. The English data set is available via DataShop:\n",
    "\n",
    "https://pslcdatashop.web.cmu.edu/DatasetInfo?datasetId=5371 <br>\n",
    "https://pslcdatashop.web.cmu.edu/DatasetInfo?datasetId=5820\n",
    "\n",
    "The German data set is not available due to data protection terms in the initial data collection, in line with relevant regulations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1d18f5-49f2-4aae-a515-085657480e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined = pd.concat([\n",
    "    pd.read_csv('think_aloud_english_chem.csv'),\n",
    "    pd.read_csv('think_aloud_english_logic.csv'),\n",
    "    pd.read_csv('German-Labels.csv')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8322448-e3a5-484d-b6ce-f7ca27dcac02",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined['sample'] = df_combined.row.map(lambda x: 'German' if pd.isna(x) else 'English')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0693a6c-61c4-4bb8-a439-0bc818ed0d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined['n_words'] = df_combined.utterance_combined.map(lambda s: len(s.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b319879-788c-4134-b72e-d466b4ae00e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "round(df_combined.groupby(['platform'])['n_words'].mean(), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55978920-9662-41c6-a310-d846f955a4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "round(df_combined.groupby(['sample'])['n_words'].mean(), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac67714-82cf-4547-a49f-8b15a16d1773",
   "metadata": {},
   "outputs": [],
   "source": [
    "round(df_combined.groupby(['sample', 'platform'])['n_words'].mean(), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fbfb925-aa5e-470c-91d1-be6ef9ff2c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_summary(f='think_aloud_english_logic.csv', filter_platform=None):\n",
    "    def get_perc(decimal):\n",
    "        return f'{round(decimal*100, 2)}%'\n",
    "    dd = pd.read_csv(f)\n",
    "    if filter_platform is not None:\n",
    "        dd = dd[dd['platform']==filter_platform].copy()\n",
    "    for var in ['process', 'plan', 'act', 'wrong']:\n",
    "        dd[var] = dd[var].map(lambda x: x if not isinstance(x, str) else 1 if x=='Yes' else 0)\n",
    "    n = dd.shape[0]\n",
    "    print(f'N Utterances: {n}')\n",
    "    print(f'% Process: {get_perc(sum(dd.process.values==1)/n)}')\n",
    "    print(f'% Plan: {get_perc(sum(dd.plan.values==1)/n)}')\n",
    "    print(f'% Enact: {get_perc(sum(dd.act.values==1)/n)}')\n",
    "    print(f'% Realizing errors: {get_perc(sum(dd.wrong.values==1)/n)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c2a324-64ce-4d5e-bb71-f44fe8ac35a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_summary('think_aloud_english_chem.csv', filter_platform='Stoich')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72aecae1-b20c-4bdc-800e-bd4e995649ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_summary('think_aloud_english_chem.csv', filter_platform='ORCCA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d51a301-ed3d-4709-9cdc-46582fdfa425",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_summary('think_aloud_english_logic.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862280c3-df5a-4497-8fce-1cea550e1afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_summary('German-Labels.csv', filter_platform='Stoich')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "422fcb26-afda-4b9a-9dc8-e99adc598c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_summary('German-Labels.csv', filter_platform='ORCCA')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9b5883",
   "metadata": {},
   "source": [
    "### OpenAI Embedding Model\n",
    "\n",
    "The following code creates procedures to text embeddings of student utterances which are latter used as model input features.\n",
    "\n",
    "In this example, we create and store embeddings in the following manner to avoid recomputing embeddings each time:\n",
    "\n",
    "\n",
    "```\n",
    "text = df['utterance_combined']\n",
    "\n",
    "X = []\n",
    "for r in tqdm(text):\n",
    "    emb = get_embedding(r)\n",
    "    review_emb = tf.reshape(emb, [-1]).numpy()\n",
    "    X.append(review_emb)\n",
    "X = np.array(X)\n",
    "np.save('file.npy', X)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2936621d-3e44-4196-8022-c0a9b8b9f9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_key(path='token_conrad.txt'):\n",
    "    \"\"\"Reads the API key from token.txt.\"\"\"\n",
    "    with open(path, 'r') as file:\n",
    "        api_key = file.read().strip()\n",
    "    return api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed439b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = read_key(path='token_conrad.txt')\n",
    "\n",
    "def get_embedding(text, model=\"text-embedding-3-small\"):\n",
    "   text = text.replace(\"\\n\", \" \")\n",
    "   return openai.Embedding.create(input = [text], model=model)['data'][0]['embedding']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb85a342-4449-4f92-acfa-e0696118670d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('think_aloud_english_chem.csv')\n",
    "#df = pd.read_csv(\"German-Labels.csv\")\n",
    "#df = pd.read_csv(\"think_aloud_all_platforms_2023Nov27.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e8a8f9",
   "metadata": {},
   "source": [
    "### Machine Learning SRL Prediction Model\n",
    "\n",
    "Below, we run the model training and evaluation procedure described in our paper for each data subset separately (e.g., all English data or all Chemistry data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2c6fb8-b95f-4620-bec9-46bb99fedbee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_data(f=\"think_aloud_all_platforms_2023Nov27.csv\", student_col='anon_student_id'):\n",
    "    df = pd.read_csv(f) # this csv contains all valid input, in attempt level\n",
    "    for var in ['process', 'plan', 'act', 'wrong']:\n",
    "        df[var] = df[var].map(lambda x: x if not isinstance(x, str) else 1 if x=='Yes' else 0)\n",
    "    \n",
    "    group_dict = dict()\n",
    "    groups = np.array([])\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        s_id = row[student_col]\n",
    "        if s_id not in group_dict:\n",
    "            group_dict[s_id] = index\n",
    "        groups = np.append(groups, group_dict[s_id])\n",
    "        \n",
    "    # Set up the splitter with 5 splits\n",
    "    gkf = GroupKFold(n_splits = 5)\n",
    "    return df, gkf, groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c146128f-d474-49ef-9f6f-022ff3a424d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(df, gkf, groups, X, label='wrong', modelfile='english-te3-wrong.h5'):\n",
    "    y = df[label]\n",
    "    # set up storage arrays for each round of validation\n",
    "    roc_auc_scores_all = np.array([])\n",
    "    pred = pd.DataFrame()\n",
    "    \n",
    "    for train_index, test_index in gkf.split(X, y, groups=groups):\n",
    "        \n",
    "        # Get the training and test data from the dataset for this group\n",
    "        X_train = X[train_index]\n",
    "        X_test = X[test_index]\n",
    "        y_train = y.iloc[train_index]\n",
    "        y_test = y.iloc[test_index]\n",
    "        \n",
    "        model = Sequential()\n",
    "        model.add(Dense(28, input_shape=(1536,), activation='relu')) \n",
    "        model.add(Dense(28, activation='relu')) \n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "        \n",
    "        optimizer = tf.keras.optimizers.legacy.Adam(lr=0.01)\n",
    "        model.compile(loss = 'binary_crossentropy', optimizer= optimizer,metrics = ['acc'])\n",
    "        \n",
    "        num_epochs = 30\n",
    "        batch_size = 10\n",
    "    \n",
    "        model.fit(\n",
    "            X_train, \n",
    "            y_train, \n",
    "            epochs=num_epochs, \n",
    "            validation_split=0.1,\n",
    "            shuffle=True, \n",
    "            batch_size=batch_size, \n",
    "            verbose=0)\n",
    "        \n",
    "        # test classifier on this round of testing group\n",
    "        predictions = model.predict(X_test)\n",
    "        \n",
    "        pred_new = pd.concat([\n",
    "        pd.DataFrame(df.iloc[test_index,[0]]).reset_index(drop=True), #row number\n",
    "        pd.DataFrame(y.iloc[test_index]).reset_index(drop=True),\n",
    "        pd.DataFrame(predictions).reset_index(drop=True)],ignore_index=True, axis = 1)\n",
    "        \n",
    "        pred = pred.append(pred_new, ignore_index=True)\n",
    "        \n",
    "        # compute some metrics and store them for averaging later on\n",
    "       \n",
    "        # AUC\n",
    "        roc_auc_scores = roc_auc_score(y_test, predictions)\n",
    "        roc_auc_scores_all = np.append(roc_auc_scores_all, roc_auc_scores)\n",
    "\n",
    "    # Train model on full data set and save it for later\n",
    "    model.fit(\n",
    "        X, \n",
    "        y, \n",
    "        epochs=num_epochs, \n",
    "        validation_split=0.1,\n",
    "        shuffle=True, \n",
    "        batch_size=batch_size, \n",
    "        verbose=0)\n",
    "    model.save(modelfile)\n",
    "    \n",
    "    return pred, roc_auc_scores_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11d94c1-57af-4c01-8d39-47b00ed07b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_procedure(f_df, f_X, student_col='anon_student_id', label='act', modelfile='tmp.h5'):\n",
    "    df, gkf, groups = prep_data(f_df, student_col)\n",
    "    X = np.load(f_X)\n",
    "    pred_english_wrong, auc_english_wrong = train_model(df, gkf, groups, X, label=label, modelfile=modelfile)\n",
    "    return pred_english_wrong, auc_english_wrong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20bb87ed-ee1d-46b8-a6c9-460a670b3920",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pairs = [\n",
    "    ('think_aloud_all_platforms_2023Nov27.csv', 'X-text-embedding-small-955-English.npy', 'anon_student_id', 'english-all'),\n",
    "    ('think_aloud_english_chem.csv', 'X-text-embedding-small-631-English-chem.npy', 'anon_student_id', 'english-chem'),\n",
    "    ('think_aloud_english_logic.csv', 'X-text-embedding-small-324-English-logic.npy', 'anon_student_id', 'english-logic'),\n",
    "    ('German-Labels.csv', 'X-text-embedding-small-584-German.npy', 'user', 'german-chem'),\n",
    "    ('think_chem_all_languages.csv', 'X-text-embedding-small-1215-chem-alldata.npy', 'anon_student_id', 'all-chem'),\n",
    "    ('think_all_all_platforms_all_languages.csv', 'X-text-embedding-small-1539-alldata.npy', 'anon_student_id', 'all-all')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8beb9d50-10be-423b-a6dd-39bf6a6c6ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_results = []\n",
    "for f_df, f_X, student_col, ref in model_pairs:\n",
    "    for label in ['process', 'plan', 'act', 'wrong']:  \n",
    "        reference = ref+'-'+label\n",
    "        pred_english_wrong, auc_english_wrong = train_procedure(f_df, f_X, student_col=student_col,\n",
    "                                                                label=label, modelfile=f'{reference}.h5')\n",
    "        model_results.append((reference, pred_english_wrong, auc_english_wrong))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6d7ecf-51df-4cd7-9779-5266f0347af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cv_res = pd.concat([pd.DataFrame([{'ref': ref, 'auc_mean': np.round(aucs.mean(), 3), 'auc_std': np.round(aucs.std(), 3)}]) for ref, _, aucs in model_results])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62023d43-cb7b-4ce4-a3a7-ea3e909b114d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cv_res.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c27d0ea",
   "metadata": {},
   "source": [
    "## Transferability\n",
    "\n",
    "The same modeling procedure, using precomputed embedding files for training and testing, are used to investigate the accuracy of training a model on one subset and then applying it to another. DeLong tests are used to compare AUC values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "006d9ce2-b5f8-4dc6-b8d8-57df4c0ae98e",
   "metadata": {},
   "source": [
    "## Transfer 1: English all to German Chem vs. English Chem to German Chem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fcfd024-b237-42c7-a6ae-a5f4cef3a9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_apply_model(model_path, X_test):\n",
    "    loaded_model = load_model(model_path)\n",
    "    predictions = loaded_model.predict(X_test)    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0939287a-1e5b-474a-a5fb-4ee444c16c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_predictions_and_export(df, predictions, var='wrong'):\n",
    "    df[var+'_pred_num'] = [round(x, 3) for x in predictions.flatten()]\n",
    "    df[var+'_pred'] = [1 if x>0.5 else 0 for x in predictions.flatten()]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e744ef-5dde-4c28-bdc8-c9df6f754caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_auc_ci(labels, predictions, n_bootstraps=1000, alpha=0.05, round_digits=True):\n",
    "    \"\"\"\n",
    "    Calculate the ROC AUC score and bootstrap a 95% confidence interval.\n",
    "    \n",
    "    Parameters:\n",
    "    labels (array-like): True binary labels.\n",
    "    predictions (array-like): Target scores, can either be probability estimates of the positive class, confidence values, or binary decisions.\n",
    "    n_bootstraps (int): Number of bootstrap samples to draw.\n",
    "    alpha (float): Significance level for the CI.\n",
    "    \n",
    "    Returns:\n",
    "    tuple: (ROC AUC score, lower bound of CI, upper bound of CI)\n",
    "    \"\"\"\n",
    "    # Calculate the ROC AUC score\n",
    "    roc_auc = roc_auc_score(labels, predictions)\n",
    "    \n",
    "    # Generate bootstrap samples\n",
    "    bootstrapped_scores = []\n",
    "    for _ in range(n_bootstraps):\n",
    "        # Resample with replacement\n",
    "        indices = resample(range(len(labels)))\n",
    "        if len(np.unique(labels[indices])) < 2:\n",
    "            # Skip this sample if it does not contain both classes\n",
    "            continue\n",
    "        score = roc_auc_score(labels[indices], predictions[indices])\n",
    "        bootstrapped_scores.append(score)\n",
    "    \n",
    "    # Calculate the confidence interval\n",
    "    sorted_scores = np.sort(bootstrapped_scores)\n",
    "    lower_bound = np.percentile(sorted_scores, 100 * alpha / 2)\n",
    "    upper_bound = np.percentile(sorted_scores, 100 * (1 - alpha / 2))\n",
    "\n",
    "    if round_digits:\n",
    "        roc_auc = f\"{roc_auc:.{3}f}\"\n",
    "        lower_bound = f\"{lower_bound:.{3}f}\"\n",
    "        upper_bound = f\"{upper_bound:.{3}f}\"\n",
    "        return f'AUC = {roc_auc}, CI95% = [{lower_bound}, {upper_bound}]'\n",
    "    \n",
    "    return roc_auc, lower_bound, upper_bound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7c0b56-2e0d-4526-a314-5b8a59f218b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "from scipy.stats import norm\n",
    "import numpy as np\n",
    "\n",
    "def auc_confidence_interval(y_true, y_scores, alpha=0.05, round_digits=True):\n",
    "    \"\"\"\n",
    "    Calculate the ROC AUC score and its confidence interval using the DeLong method.\n",
    "    \n",
    "    Parameters:\n",
    "    y_true (array-like): True binary labels.\n",
    "    y_scores (array-like): Target scores, can either be probability estimates of the positive class, confidence values, or binary decisions.\n",
    "    alpha (float): Significance level for the CI.\n",
    "    \n",
    "    Returns:\n",
    "    tuple: (ROC AUC score, lower bound of CI, upper bound of CI)\n",
    "    \"\"\"\n",
    "    # Compute the ROC AUC score\n",
    "    auc = roc_auc_score(y_true, y_scores)\n",
    "    \n",
    "    # Calculate AUC variance using the DeLong method\n",
    "    n1 = sum(y_true)\n",
    "    n2 = len(y_true) - n1\n",
    "    q1 = auc / (2 - auc)\n",
    "    q2 = 2 * auc ** 2 / (1 + auc)\n",
    "\n",
    "    auc_var = (auc * (1 - auc) + (n1 - 1) * (q1 - auc ** 2) + (n2 - 1) * (q2 - auc ** 2)) / (n1 * n2)\n",
    "    auc_std = np.sqrt(auc_var)\n",
    "\n",
    "    # Calculate the confidence interval\n",
    "    z = norm.ppf(1 - alpha / 2)\n",
    "    lower_bound = auc - z * auc_std\n",
    "    upper_bound = auc + z * auc_std\n",
    "    \n",
    "    # Bound the values between 0 and 1\n",
    "    lower_bound = max(0, lower_bound)\n",
    "    upper_bound = min(1, upper_bound)\n",
    "\n",
    "    if round_digits:\n",
    "        auc = f\"{auc:.{3}f}\"\n",
    "        lower_bound = f\"{lower_bound:.{3}f}\"\n",
    "        upper_bound = f\"{upper_bound:.{3}f}\"\n",
    "        return f'AUC = {auc}, CI95% = [{lower_bound}, {upper_bound}]'\n",
    "    \n",
    "    return auc, lower_bound, upper_bound\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c40e53-3a59-4d7b-9959-7c0b10570158",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats\n",
    "from scipy import stats\n",
    "\n",
    "# AUC comparison adapted from\n",
    "# https://github.com/Netflix/vmaf/\n",
    "def compute_midrank(x):\n",
    "    \"\"\"Computes midranks.\n",
    "    Args:\n",
    "       x - a 1D numpy array\n",
    "    Returns:\n",
    "       array of midranks\n",
    "    \"\"\"\n",
    "    J = np.argsort(x)\n",
    "    Z = x[J]\n",
    "    N = len(x)\n",
    "    T = np.zeros(N, dtype=float)\n",
    "    i = 0\n",
    "    while i < N:\n",
    "        j = i\n",
    "        while j < N and Z[j] == Z[i]:\n",
    "            j += 1\n",
    "        T[i:j] = 0.5*(i + j - 1)\n",
    "        i = j\n",
    "    T2 = np.empty(N, dtype=float)\n",
    "    # Note(kazeevn) +1 is due to Python using 0-based indexing\n",
    "    # instead of 1-based in the AUC formula in the paper\n",
    "    T2[J] = T + 1\n",
    "    return T2\n",
    "\n",
    "\n",
    "def compute_midrank_weight(x, sample_weight):\n",
    "    \"\"\"Computes midranks.\n",
    "    Args:\n",
    "       x - a 1D numpy array\n",
    "    Returns:\n",
    "       array of midranks\n",
    "    \"\"\"\n",
    "    J = np.argsort(x)\n",
    "    Z = x[J]\n",
    "    cumulative_weight = np.cumsum(sample_weight[J])\n",
    "    N = len(x)\n",
    "    T = np.zeros(N, dtype=float)\n",
    "    i = 0\n",
    "    while i < N:\n",
    "        j = i\n",
    "        while j < N and Z[j] == Z[i]:\n",
    "            j += 1\n",
    "        T[i:j] = cumulative_weight[i:j].mean()\n",
    "        i = j\n",
    "    T2 = np.empty(N, dtype=float)\n",
    "    T2[J] = T\n",
    "    return T2\n",
    "\n",
    "\n",
    "def fastDeLong(predictions_sorted_transposed, label_1_count, sample_weight):\n",
    "    if sample_weight is None:\n",
    "        return fastDeLong_no_weights(predictions_sorted_transposed, label_1_count)\n",
    "    else:\n",
    "        return fastDeLong_weights(predictions_sorted_transposed, label_1_count, sample_weight)\n",
    "\n",
    "\n",
    "def fastDeLong_weights(predictions_sorted_transposed, label_1_count, sample_weight):\n",
    "    \"\"\"\n",
    "    The fast version of DeLong's method for computing the covariance of\n",
    "    unadjusted AUC.\n",
    "    Args:\n",
    "       predictions_sorted_transposed: a 2D numpy.array[n_classifiers, n_examples]\n",
    "          sorted such as the examples with label \"1\" are first\n",
    "    Returns:\n",
    "       (AUC value, DeLong covariance)\n",
    "    Reference:\n",
    "     @article{sun2014fast,\n",
    "       title={Fast Implementation of DeLong's Algorithm for\n",
    "              Comparing the Areas Under Correlated Receiver Oerating Characteristic Curves},\n",
    "       author={Xu Sun and Weichao Xu},\n",
    "       journal={IEEE Signal Processing Letters},\n",
    "       volume={21},\n",
    "       number={11},\n",
    "       pages={1389--1393},\n",
    "       year={2014},\n",
    "       publisher={IEEE}\n",
    "     }\n",
    "    \"\"\"\n",
    "    # Short variables are named as they are in the paper\n",
    "    m = label_1_count\n",
    "    n = predictions_sorted_transposed.shape[1] - m\n",
    "    positive_examples = predictions_sorted_transposed[:, :m]\n",
    "    negative_examples = predictions_sorted_transposed[:, m:]\n",
    "    k = predictions_sorted_transposed.shape[0]\n",
    "\n",
    "    tx = np.empty([k, m], dtype=float)\n",
    "    ty = np.empty([k, n], dtype=float)\n",
    "    tz = np.empty([k, m + n], dtype=float)\n",
    "    for r in range(k):\n",
    "        tx[r, :] = compute_midrank_weight(positive_examples[r, :], sample_weight[:m])\n",
    "        ty[r, :] = compute_midrank_weight(negative_examples[r, :], sample_weight[m:])\n",
    "        tz[r, :] = compute_midrank_weight(predictions_sorted_transposed[r, :], sample_weight)\n",
    "    total_positive_weights = sample_weight[:m].sum()\n",
    "    total_negative_weights = sample_weight[m:].sum()\n",
    "    pair_weights = np.dot(sample_weight[:m, np.newaxis], sample_weight[np.newaxis, m:])\n",
    "    total_pair_weights = pair_weights.sum()\n",
    "    aucs = (sample_weight[:m]*(tz[:, :m] - tx)).sum(axis=1) / total_pair_weights\n",
    "    v01 = (tz[:, :m] - tx[:, :]) / total_negative_weights\n",
    "    v10 = 1. - (tz[:, m:] - ty[:, :]) / total_positive_weights\n",
    "    sx = np.cov(v01)\n",
    "    sy = np.cov(v10)\n",
    "    delongcov = sx / m + sy / n\n",
    "    return aucs, delongcov\n",
    "\n",
    "\n",
    "def fastDeLong_no_weights(predictions_sorted_transposed, label_1_count):\n",
    "    \"\"\"\n",
    "    The fast version of DeLong's method for computing the covariance of\n",
    "    unadjusted AUC.\n",
    "    Args:\n",
    "       predictions_sorted_transposed: a 2D numpy.array[n_classifiers, n_examples]\n",
    "          sorted such as the examples with label \"1\" are first\n",
    "    Returns:\n",
    "       (AUC value, DeLong covariance)\n",
    "    Reference:\n",
    "     @article{sun2014fast,\n",
    "       title={Fast Implementation of DeLong's Algorithm for\n",
    "              Comparing the Areas Under Correlated Receiver Oerating\n",
    "              Characteristic Curves},\n",
    "       author={Xu Sun and Weichao Xu},\n",
    "       journal={IEEE Signal Processing Letters},\n",
    "       volume={21},\n",
    "       number={11},\n",
    "       pages={1389--1393},\n",
    "       year={2014},\n",
    "       publisher={IEEE}\n",
    "     }\n",
    "    \"\"\"\n",
    "    # Short variables are named as they are in the paper\n",
    "    m = label_1_count\n",
    "    n = predictions_sorted_transposed.shape[1] - m\n",
    "    positive_examples = predictions_sorted_transposed[:, :m]\n",
    "    negative_examples = predictions_sorted_transposed[:, m:]\n",
    "    k = predictions_sorted_transposed.shape[0]\n",
    "\n",
    "    tx = np.empty([k, m], dtype=float)\n",
    "    ty = np.empty([k, n], dtype=float)\n",
    "    tz = np.empty([k, m + n], dtype=float)\n",
    "    for r in range(k):\n",
    "        tx[r, :] = compute_midrank(positive_examples[r, :])\n",
    "        ty[r, :] = compute_midrank(negative_examples[r, :])\n",
    "        tz[r, :] = compute_midrank(predictions_sorted_transposed[r, :])\n",
    "    aucs = tz[:, :m].sum(axis=1) / m / n - float(m + 1.0) / 2.0 / n\n",
    "    v01 = (tz[:, :m] - tx[:, :]) / n\n",
    "    v10 = 1.0 - (tz[:, m:] - ty[:, :]) / m\n",
    "    sx = np.cov(v01)\n",
    "    sy = np.cov(v10)\n",
    "    delongcov = sx / m + sy / n\n",
    "    return aucs, delongcov\n",
    "\n",
    "\n",
    "def calc_pvalue(aucs, sigma):\n",
    "    \"\"\"Computes log(10) of p-values.\n",
    "    Args:\n",
    "       aucs: 1D array of AUCs\n",
    "       sigma: AUC DeLong covariances\n",
    "    Returns:\n",
    "       log10(pvalue)\n",
    "    \"\"\"\n",
    "    l = np.array([[1, -1]])\n",
    "    z = np.abs(np.diff(aucs)) / np.sqrt(np.dot(np.dot(l, sigma), l.T))\n",
    "    return np.log10(2) + scipy.stats.norm.logsf(z, loc=0, scale=1) / np.log(10)\n",
    "\n",
    "\n",
    "def compute_ground_truth_statistics(ground_truth, sample_weight):\n",
    "    assert np.array_equal(np.unique(ground_truth), [0, 1])\n",
    "    order = (-ground_truth).argsort()\n",
    "    label_1_count = int(ground_truth.sum())\n",
    "    if sample_weight is None:\n",
    "        ordered_sample_weight = None\n",
    "    else:\n",
    "        ordered_sample_weight = sample_weight[order]\n",
    "\n",
    "    return order, label_1_count, ordered_sample_weight\n",
    "\n",
    "\n",
    "def delong_roc_variance(ground_truth, predictions, sample_weight=None):\n",
    "    \"\"\"\n",
    "    Computes ROC AUC variance for a single set of predictions\n",
    "    Args:\n",
    "       ground_truth: np.array of 0 and 1\n",
    "       predictions: np.array of floats of the probability of being class 1\n",
    "    \"\"\"\n",
    "    order, label_1_count, ordered_sample_weight = compute_ground_truth_statistics(\n",
    "        ground_truth, sample_weight)\n",
    "    predictions_sorted_transposed = predictions[np.newaxis, order]\n",
    "    aucs, delongcov = fastDeLong(predictions_sorted_transposed, label_1_count, ordered_sample_weight)\n",
    "    assert len(aucs) == 1, \"There is a bug in the code, please forward this to the developers\"\n",
    "    return aucs[0], delongcov\n",
    "\n",
    "\n",
    "def delong_auc_ci(y_true, y_pred, alpha=.95):\n",
    "    auc, auc_cov = delong_roc_variance(\n",
    "        y_true,\n",
    "        y_pred)\n",
    "    \n",
    "    auc_std = np.sqrt(auc_cov)\n",
    "    lower_upper_q = np.abs(np.array([0, 1]) - (1 - alpha) / 2)\n",
    "    \n",
    "    ci = stats.norm.ppf(\n",
    "        lower_upper_q,\n",
    "        loc=auc,\n",
    "        scale=auc_std)\n",
    "    \n",
    "    ci[ci > 1] = 1\n",
    "    \n",
    "    return f'{auc}: {ci}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2243b192-44da-4406-b6e9-2e60902c95a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_df = 'German-Labels.csv'\n",
    "f_X = 'X-text-embedding-small-584-German.npy'\n",
    "student_col='user'\n",
    "df, gkf, groups = prep_data(f_df, student_col)\n",
    "X = np.load(f_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa8cbf4-6cc5-409d-b75d-0ed5301d0da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "transfer_aucs = []\n",
    "df_out = pd.read_csv(f_df)\n",
    "for var in ['process', 'plan', 'act', 'wrong']:\n",
    "    df_out[var] = df_out[var].map(lambda x: x if not isinstance(x, str) else 1 if x=='Yes' else 0)\n",
    "for label in ['process', 'plan', 'act', 'wrong']:  \n",
    "    f_model = f'german-chem-{label}.h5'\n",
    "    predictions = load_and_apply_model(f_model.replace('german-chem', 'english-all'), X)\n",
    "    df_out = add_predictions_and_export(df_out, predictions, var=label)\n",
    "    transfer_aucs.append((f'german-chem-{label}-using-english-all', bootstrap_auc_ci(df[label], predictions)))\n",
    "for label in ['process', 'plan', 'act', 'wrong']:  \n",
    "    df_out[df_out[label]!=df_out[f'{label}_pred']][[c for c in df_out if label in c or c=='utterance_combined' or c=='platform']].to_csv(f_df.replace('.csv', f'-with-predictions-using-english-all-inconsistent-{label}.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a84078-0327-4cc8-9968-9f06293a2d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([pd.DataFrame([{'ref': ref, 'auc': auc}]) for ref, auc in transfer_aucs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f722d2d9-e4d4-4417-a459-57eddd8656be",
   "metadata": {},
   "outputs": [],
   "source": [
    "transfer_aucs = []\n",
    "df_out = pd.read_csv(f_df)\n",
    "for var in ['process', 'plan', 'act', 'wrong']:\n",
    "    df_out[var] = df_out[var].map(lambda x: x if not isinstance(x, str) else 1 if x=='Yes' else 0)\n",
    "for label in ['process', 'plan', 'act', 'wrong']:  \n",
    "    f_model = f'german-chem-{label}.h5'\n",
    "    predictions = load_and_apply_model(f_model.replace('german-chem', 'english-chem'), X)\n",
    "    df_out = add_predictions_and_export(df_out, predictions, var=label)\n",
    "    transfer_aucs.append((f'german-chem-{label}-using-english-chem', delong_auc_ci(df[label], predictions.flatten())))\n",
    "for label in ['process', 'plan', 'act', 'wrong']:  \n",
    "    df_out[df_out[label]!=df_out[f'{label}_pred']][[c for c in df_out if label in c or c=='utterance_combined' or c=='platform']].to_csv(f_df.replace('.csv', f'-with-predictions-using-english-chem-inconsistent-{label}.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58a2c3a-3f14-49fd-8d12-71b31f723fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([pd.DataFrame([{'ref': ref, 'auc': auc}]) for ref, auc in transfer_aucs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b5d73b-e151-4c68-93a2-3429a1d9e87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_df = 'German-Labels.csv'\n",
    "f_X = 'X-text-embedding-small-584-German.npy'\n",
    "student_col='user'\n",
    "df, gkf, groups = prep_data(f_df, student_col)\n",
    "X = np.load(f_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190204b9-006e-480b-add9-adc11601a978",
   "metadata": {},
   "outputs": [],
   "source": [
    "transfer_aucs = []\n",
    "df_out = pd.read_csv(f_df)\n",
    "for var in ['process', 'plan', 'act', 'wrong']:\n",
    "    df_out[var] = df_out[var].map(lambda x: x if not isinstance(x, str) else 1 if x=='Yes' else 0)\n",
    "for label in ['process', 'plan', 'act', 'wrong']:  \n",
    "    f_model = f'german-chem-{label}.h5'\n",
    "    predictions = load_and_apply_model(f_model, X)\n",
    "    df_out = add_predictions_and_export(df_out, predictions, var=label)\n",
    "    transfer_aucs.append((f'german-chem-{label}-using-german-chem', delong_auc_ci(df[label], predictions.flatten())))\n",
    "for label in ['process', 'plan', 'act', 'wrong']:  \n",
    "    df_out[df_out[label]!=df_out[f'{label}_pred']][[c for c in df_out if label in c or c=='utterance_combined' or c=='platform']].to_csv(f_df.replace('.csv', f'-with-predictions-using-german-chem-inconsistent-{label}.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd73770-4582-49e6-97b6-3b097ad68113",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([pd.DataFrame([{'ref': ref, 'auc': auc}]) for ref, auc in transfer_aucs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40dcede-25a6-4cb2-93dc-837292d230e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "transfer_aucs = []\n",
    "df_out = pd.read_csv(f_df)\n",
    "for var in ['process', 'plan', 'act', 'wrong']:\n",
    "    df_out[var] = df_out[var].map(lambda x: x if not isinstance(x, str) else 1 if x=='Yes' else 0)\n",
    "for label in ['process', 'plan', 'act', 'wrong']:  \n",
    "    f_model = f'german-chem-{label}.h5'\n",
    "    predictions = load_and_apply_model(f_model.replace('german-chem', 'english-logic'), X)\n",
    "    df_out = add_predictions_and_export(df_out, predictions, var=label)\n",
    "    transfer_aucs.append((f'german-chem-{label}-using-english-logic', delong_auc_ci(df[label], predictions.flatten())))\n",
    "for label in ['process', 'plan', 'act', 'wrong']:  \n",
    "    df_out[df_out[label]!=df_out[f'{label}_pred']][[c for c in df_out if label in c or c=='utterance_combined' or c=='platform']].to_csv(f_df.replace('.csv', f'-with-predictions-using-english-logic-inconsistent-{label}.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f86aab-83aa-4892-816c-08639c50d8a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([pd.DataFrame([{'ref': ref, 'auc': auc}]) for ref, auc in transfer_aucs])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0310766-349d-45d5-b098-ac1a5df57c24",
   "metadata": {},
   "source": [
    "# Generalizablity by platform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a215ea3-fbcb-4f6a-88a4-27eae2e41310",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_df = 'english-stoich.csv'\n",
    "f_X = 'X-text-embedding-small-469-English-Stoich.npy'\n",
    "student_col='anon_student_id'\n",
    "df, gkf, groups = prep_data(f_df, student_col)\n",
    "X = np.load(f_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68b5b5c-7155-4a52-921b-f263c88fa6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "transfer_aucs = []\n",
    "df_out = pd.read_csv(f_df)\n",
    "for var in ['process', 'plan', 'act', 'wrong']:\n",
    "    df_out[var] = df_out[var].map(lambda x: x if not isinstance(x, str) else 1 if x=='Yes' else 0)\n",
    "for label in ['process', 'plan', 'act', 'wrong']:  \n",
    "    f_model = f'english-chem-{label}.h5'\n",
    "    predictions = load_and_apply_model(f_model.replace('english-chem', 'german-chem'), X)\n",
    "    df_out = add_predictions_and_export(df_out, predictions, var=label)\n",
    "    transfer_aucs.append((f'english-chem-stoich-{label}-using-german-chem', delong_auc_ci(df[label], predictions.flatten())))\n",
    "for label in ['process', 'plan', 'act', 'wrong']:  \n",
    "    df_out[df_out[label]!=df_out[f'{label}_pred']][[c for c in df_out if label in c or c=='utterance_combined' or c=='platform']].to_csv(f_df.replace('.csv', f'-with-predictions-using-german-chem-inconsistent-{label}.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d68941-5f38-4e6c-83cc-3167718b57bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([pd.DataFrame([{'ref': ref, 'auc': auc}]) for ref, auc in transfer_aucs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380414d0-dd9e-4c4e-9e6d-4a397599d188",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_df = 'english-orcca.csv'\n",
    "f_X = 'X-text-embedding-small-162-English-ORCCA.npy'\n",
    "student_col='anon_student_id'\n",
    "df, gkf, groups = prep_data(f_df, student_col)\n",
    "X = np.load(f_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7485d0f5-f97b-44e8-9c7a-fa18710e697f",
   "metadata": {},
   "outputs": [],
   "source": [
    "transfer_aucs = []\n",
    "df_out = pd.read_csv(f_df)\n",
    "for var in ['process', 'plan', 'act', 'wrong']:\n",
    "    df_out[var] = df_out[var].map(lambda x: x if not isinstance(x, str) else 1 if x=='Yes' else 0)\n",
    "for label in ['process', 'plan', 'act', 'wrong']:  \n",
    "    f_model = f'english-chem-{label}.h5'\n",
    "    predictions = load_and_apply_model(f_model.replace('english-chem', 'german-chem'), X)\n",
    "    df_out = add_predictions_and_export(df_out, predictions, var=label)\n",
    "    transfer_aucs.append((f'english-chem-ORCCA-{label}-using-german-chem', delong_auc_ci(df[label], predictions.flatten())))\n",
    "for label in ['process', 'plan', 'act', 'wrong']:  \n",
    "    df_out[df_out[label]!=df_out[f'{label}_pred']][[c for c in df_out if label in c or c=='utterance_combined' or c=='platform']].to_csv(f_df.replace('.csv', f'-with-predictions-using-german-chem-inconsistent-{label}.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833ecb66-f0f8-4672-8c7a-6937fe213fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([pd.DataFrame([{'ref': ref, 'auc': auc}]) for ref, auc in transfer_aucs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c540fb-55ea-434a-9ed8-572b929d89ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_df = 'german-stoich.csv'\n",
    "f_X = 'X-text-embedding-small-439-German-Stoich.npy'\n",
    "student_col='user'\n",
    "df, gkf, groups = prep_data(f_df, student_col)\n",
    "X = np.load(f_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7459b0d-321f-4007-8ef3-7f0789b96ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "transfer_aucs = []\n",
    "df_out = pd.read_csv(f_df)\n",
    "for var in ['process', 'plan', 'act', 'wrong']:\n",
    "    df_out[var] = df_out[var].map(lambda x: x if not isinstance(x, str) else 1 if x=='Yes' else 0)\n",
    "for label in ['process', 'plan', 'act', 'wrong']:  \n",
    "    f_model = f'german-chem-{label}.h5'\n",
    "    predictions = load_and_apply_model(f_model.replace('german-chem', 'english-chem'), X)\n",
    "    df_out = add_predictions_and_export(df_out, predictions, var=label)\n",
    "    transfer_aucs.append((f'german-chem-stoich-{label}-using-english-chem', delong_auc_ci(df[label], predictions.flatten())))\n",
    "for label in ['process', 'plan', 'act', 'wrong']:  \n",
    "    df_out[df_out[label]!=df_out[f'{label}_pred']][[c for c in df_out if label in c or c=='utterance_combined' or c=='platform']].to_csv(f_df.replace('.csv', f'-with-predictions-using-english-chem-inconsistent-{label}.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a173dd3-1587-4bb8-af75-a62c36ac1901",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([pd.DataFrame([{'ref': ref, 'auc': auc}]) for ref, auc in transfer_aucs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d89def4-77f8-482d-b467-feca8fdd0785",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_df = 'german-ORCCA.csv'\n",
    "f_X = 'X-text-embedding-small-145-German-ORCCA.npy'\n",
    "student_col='user'\n",
    "df, gkf, groups = prep_data(f_df, student_col)\n",
    "X = np.load(f_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5589cc8-6d45-42bf-9d09-8b5b156902d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "transfer_aucs = []\n",
    "df_out = pd.read_csv(f_df)\n",
    "for var in ['process', 'plan', 'act', 'wrong']:\n",
    "    df_out[var] = df_out[var].map(lambda x: x if not isinstance(x, str) else 1 if x=='Yes' else 0)\n",
    "for label in ['process', 'plan', 'act', 'wrong']:  \n",
    "    f_model = f'german-chem-{label}.h5'\n",
    "    predictions = load_and_apply_model(f_model.replace('german-chem', 'english-chem'), X)\n",
    "    df_out = add_predictions_and_export(df_out, predictions, var=label)\n",
    "    transfer_aucs.append((f'german-chem-ORCCA-{label}-using-english-chem', delong_auc_ci(df[label], predictions.flatten())))\n",
    "for label in ['process', 'plan', 'act', 'wrong']:  \n",
    "    df_out[df_out[label]!=df_out[f'{label}_pred']][[c for c in df_out if label in c or c=='utterance_combined' or c=='platform']].to_csv(f_df.replace('.csv', f'-with-predictions-using-english-chem-inconsistent-{label}.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b27e18-8b88-4995-b69d-0c303febdbbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([pd.DataFrame([{'ref': ref, 'auc': auc}]) for ref, auc in transfer_aucs])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c4e0f70-d578-4df9-939f-63d366740dbd",
   "metadata": {},
   "source": [
    "## Transfer 2: German chem to English chem, logic, all (three transfers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc60c46-fe35-4f67-83a3-324ff7e95721",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_df = 'think_aloud_english_chem.csv'\n",
    "f_X = 'X-text-embedding-small-631-English-chem.npy'\n",
    "student_col='anon_student_id'\n",
    "df, gkf, groups = prep_data(f_df, student_col)\n",
    "X = np.load(f_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2312b2-42e8-4405-80c7-3dffeb868ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "transfer_aucs = []\n",
    "df_out = pd.read_csv(f_df)\n",
    "for var in ['process', 'plan', 'act', 'wrong']:\n",
    "    df_out[var] = df_out[var].map(lambda x: x if not isinstance(x, str) else 1 if x=='Yes' else 0)\n",
    "for label in ['process', 'plan', 'act', 'wrong']:  \n",
    "    f_model = f'german-chem-{label}.h5'\n",
    "    predictions = load_and_apply_model(f_model, X)\n",
    "    df_out = add_predictions_and_export(df_out, predictions, var=label)\n",
    "    transfer_aucs.append((f'english-chem-{label}-using-german-chem', delong_auc_ci(df[label], predictions.flatten())))\n",
    "for label in ['process', 'plan', 'act', 'wrong']:  \n",
    "    df_out[df_out[label]!=df_out[f'{label}_pred']][[c for c in df_out if label in c or c=='utterance_combined' or c=='platform']].to_csv(f_df.replace('.csv', f'-with-predictions-using-german-chem-inconsistent-{label}.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff5dab6-f330-4067-9883-a3560bba8427",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([pd.DataFrame([{'ref': ref, 'auc': auc}]) for ref, auc in transfer_aucs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1a96fb-636a-4ff9-8da3-0e522de24212",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_df = 'think_aloud_english_logic.csv'\n",
    "f_X = 'X-text-embedding-small-324-English-logic.npy'\n",
    "student_col='anon_student_id'\n",
    "df, gkf, groups = prep_data(f_df, student_col)\n",
    "X = np.load(f_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024412d0-6dca-42ee-9ef9-4e53f991042a",
   "metadata": {},
   "outputs": [],
   "source": [
    "transfer_aucs = []\n",
    "df_out = pd.read_csv(f_df)\n",
    "for var in ['process', 'plan', 'act', 'wrong']:\n",
    "    df_out[var] = df_out[var].map(lambda x: x if not isinstance(x, str) else 1 if x=='Yes' else 0)\n",
    "for label in ['process', 'plan', 'act', 'wrong']:  \n",
    "    f_model = f'german-chem-{label}.h5'\n",
    "    predictions = load_and_apply_model(f_model, X)\n",
    "    df_out = add_predictions_and_export(df_out, predictions, var=label)\n",
    "    transfer_aucs.append((f'english-logic-{label}-using-german-chem', delong_auc_ci(df[label], predictions.flatten())))\n",
    "for label in ['process', 'plan', 'act', 'wrong']:  \n",
    "    df_out[df_out[label]!=df_out[f'{label}_pred']][[c for c in df_out if label in c or c=='utterance_combined' or c=='platform']].to_csv(f_df.replace('.csv', f'-with-predictions-using-german-chem-inconsistent-{label}.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81653a1-5e22-4fe4-b07b-cf236a21d840",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([pd.DataFrame([{'ref': ref, 'auc': auc}]) for ref, auc in transfer_aucs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ba4e77-ef30-4c5b-97c3-8b362cfa47ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_df = 'think_aloud_all_platforms_2023Nov27.csv'\n",
    "f_X = 'X-text-embedding-small-955-English.npy'\n",
    "student_col='anon_student_id'\n",
    "df, gkf, groups = prep_data(f_df, student_col)\n",
    "X = np.load(f_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ce360c-f27b-4da7-a7c0-4339c23a3cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "transfer_aucs = []\n",
    "df_out = pd.read_csv(f_df)\n",
    "for var in ['process', 'plan', 'act', 'wrong']:\n",
    "    df_out[var] = df_out[var].map(lambda x: x if not isinstance(x, str) else 1 if x=='Yes' else 0)\n",
    "for label in ['process', 'plan', 'act', 'wrong']:  \n",
    "    f_model = f'german-chem-{label}.h5'\n",
    "    predictions = load_and_apply_model(f_model, X)\n",
    "    df_out = add_predictions_and_export(df_out, predictions, var=label)\n",
    "    transfer_aucs.append((f'english-all-{label}-using-german-chem', delong_auc_ci(df[label], predictions.flatten())))\n",
    "for label in ['process', 'plan', 'act', 'wrong']:  \n",
    "    df_out[df_out[label]!=df_out[f'{label}_pred']][[c for c in df_out if label in c or c=='utterance_combined' or c=='platform']].to_csv(f_df.replace('.csv', f'-with-predictions-using-german-chem-inconsistent-{label}.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad15ac4-37f4-4f6f-aec6-0ff57bea9cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([pd.DataFrame([{'ref': ref, 'auc': auc}]) for ref, auc in transfer_aucs])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
